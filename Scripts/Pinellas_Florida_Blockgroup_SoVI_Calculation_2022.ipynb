{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b055f246-c96b-4935-985d-497affd213a5",
   "metadata": {},
   "source": [
    "# Social Vulnerability Index (SoVI)\n",
    "\n",
    "This notebook script is designed to process and analyze census data for Pinellas County, Florida, focusing on calculating Social Vulnerability Index (SoVI) scores at the block group level using various demographic, socioeconomic, and housing variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd7ce9-e8fd-4b1e-b29e-772c33da1d54",
   "metadata": {},
   "source": [
    "# Install Necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cecc22-fb30-4be7-a077-e1a8e59d8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install census\n",
    "# pip install us\n",
    "# pip install pandas\n",
    "# pip install geopandas\n",
    "# pip install numpy\n",
    "# pip install contextily\n",
    "# pip install pyproj\n",
    "# pip install matplotlib\n",
    "# pip install seaborn\n",
    "# pip install scikit-learn\n",
    "# pip install factor_analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf2c59-e394-40bf-b70b-3c0622621c9e",
   "metadata": {},
   "source": [
    "# Load packages\n",
    "\n",
    "An instance of the Census API is initialized with an API key to fetch data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210ed61-f795-400d-9e5b-56aa7e859adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from census import Census\n",
    "from us import states\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import contextily as cx\n",
    "import proj as prj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b033ee9-06ed-475f-ab7c-4bfcf17a073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Census(\"Paste your api key here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4cb8c-bb51-487c-a01b-9f73f026cc9d",
   "metadata": {},
   "source": [
    "# Defining Variables and Fetching Census Data\n",
    "\n",
    "A set of census variables (fields) related to population, age, income, education, employment, housing, and race is defined. The script fetches data from the 2022 ACS 5-year estimates for all block groups in Pinellas County, Florida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4842982d-676e-409d-85c9-f3400b9cafce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = list({\n",
    "    'B01001_001E', # 'Total_population',\n",
    "    'B01001_026E', # 'Females',    \n",
    "    'B01001_003E', # 'male Population_under_five',\n",
    "    'B01001_027E', # 'female Population_under_five',\n",
    "    'B01001_018E', # 'male Population_over_60_61',\n",
    "    'B01001_019E', # 'male Population_over_62_64',\n",
    "    'B01001_020E', # 'male Population_over_65_66',\n",
    "    'B01001_021E', # 'male Population_over_67_69',\n",
    "    'B01001_022E', # 'male Population_over_70_74',\n",
    "    'B01001_023E', # 'male Population_over_75_79',\n",
    "    'B01001_024E', # 'male Population_over_80_84',\n",
    "    'B01001_025E', # 'male Population_over_85',\n",
    "    'B01001_042E', # 'female Population_over_60_61',\n",
    "    'B01001_043E', # 'female Population_over_62_64',\n",
    "    'B01001_044E', # 'female Population_over_65_66',\n",
    "    'B01001_045E', # 'female Population_over_67_69',\n",
    "    'B01001_046E', # 'female Population_over_70_74',\n",
    "    'B01001_047E', # 'female Population_over_75_79',\n",
    "    'B01001_048E', # 'female Population_over_80_84',\n",
    "    'B01001_049E', # 'female Population_over_85',  \n",
    "    'B11001_001E', # 'Number_of_households',\n",
    "    'B11001_006E', # 'Female_headed_households_no_spouse_present',    \n",
    "    'B19001_013E', # 'Households_earning_75K_100k',\n",
    "    'B19001_014E', # 'Households_earning_100k_125k',\n",
    "    'B19001_015E', # 'Households_earning_125K_150k',\n",
    "    'B19001_016E', # 'Households_earning_150k_200k',\n",
    "    'B19001_017E', # 'Households_earning_more_than_200k',\n",
    "    'B17010_002E', # 'People_living_in_poverty',\n",
    "    'B25003_001E', # 'Total Housing Units'\n",
    "    'B25003_003E', # 'Renter_occupied_housing_units',\n",
    "    'B25024_001E', # 'Total Units in Structure'\n",
    "    'B25024_010E', # 'Mobile_homes',    \n",
    "    'B15003_002E', # 'No_high_school_diploma',\n",
    "    'B15003_003E', # 'No_high_school_diploma',\n",
    "    'B15003_004E', # 'No_high_school_diploma',\n",
    "    'B15003_005E', # 'No_high_school_diploma',\n",
    "    'B15003_006E', # 'No_high_school_diploma',\n",
    "    'B15003_007E', # 'No_high_school_diploma',\n",
    "    'B15003_008E', # 'No_high_school_diploma',\n",
    "    'B15003_009E', # 'No_high_school_diploma',\n",
    "    'B15003_010E', # 'No_high_school_diploma',\n",
    "    'B15003_011E', # 'No_high_school_diploma',\n",
    "    'B15003_012E', # 'No_high_school_diploma',   \n",
    "    'B15003_013E', # 'No_high_school_diploma',    \n",
    "    'B15003_014E', # 'No_high_school_diploma',    \n",
    "    'B15003_015E', # 'No_high_school_diploma',\n",
    "    'B15003_016E', # 'No_high_school_diploma',    \n",
    "    'B23025_005E', # 'Unemployed_civilian_labor_force',        \n",
    "    'B23025_002E', # 'Labor_force_participation',    \n",
    "    'B01002_002E', # 'Median_age',\n",
    "    'B19301_001E', # 'Per_capita_income',\n",
    "    'B25077_001E', # 'Median_value_of_housing',\n",
    "    'B25064_001E', # 'Median_rent',\n",
    "    'B19055_001E', # 'Total Social Security recipents'\n",
    "    'B19055_002E', # 'Per_capita_Social_Security_recipients',    \n",
    "    'B02001_002E', # 'White',    \n",
    "    'B02001_003E', # 'Black',  \n",
    "    'B02001_004E', # 'American indian',  \n",
    "    'B02001_005E', # 'Asian',  \n",
    "    'B02001_006E', # 'Hawaiian',     \n",
    "    'B03002_012E', # 'Hispanic',\n",
    "    'C24030_001E', # 'Total Employment'\n",
    "    'C24030_003E', # 'Male Primary_extractive_industries_employment',\n",
    "    'C24030_028E', # 'Female Primary_extractive_industries_employment',\n",
    "    'C24020_036E', # 'Male Transportation_employment',\n",
    "    'C24020_072E', # 'Female Transportation_employment',\n",
    "    'C24020_019E', # 'Male Service_occupations_employment',\n",
    "    'C24020_001E', # Total occupations\n",
    "    'C24020_055E', # 'Female Service_occupations_employment',\n",
    "    'C24030_029E', # 'Female_labor_employment',\n",
    " \n",
    "})\n",
    "\n",
    "# FIPS codes\n",
    "state_fips = \"12\"   # Florida\n",
    "county_fips = \"103\" # Pinellas County\n",
    "\n",
    "# Fetch data for block groups in Pinellas County, FL, for ACS 5-year 2022\n",
    "data = c.acs5.state_county_blockgroup(\n",
    "    fields=fields,\n",
    "    state_fips=state_fips,\n",
    "    county_fips=county_fips,\n",
    "    blockgroup=\"*\",\n",
    "    year=2022 # Year of Census Data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cae0c70-93cc-431e-ab3b-8a64a8b207c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe with all variables\n",
    "data_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f402946-c9ee-4752-b767-9561d0b5f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the dataframe\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571b23a-16bf-4c47-8d89-4627e7eb7aee",
   "metadata": {},
   "source": [
    "# Data Processing and Calculations\n",
    "\n",
    "The script performs various calculations on the census data, such as calculating percentages for female population, children, elderly, unemployed, households earning above certain thresholds, poverty rate, rental rate, and other demographic characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db91ab6-fdae-4a33-be12-fc10087cb37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['avghouse'] = (data_df['B11001_001E'] / data_df['B01001_001E']) * 100\n",
    "data_df['female'] = (data_df['B01001_026E'] / data_df['B01001_001E']) * 100\n",
    "data_df['kid'] = ((data_df['B01001_003E'] + data_df['B01001_027E']) / data_df['B01001_001E']) * 100\n",
    "data_df['elderly'] = ((data_df['B01001_018E'] + data_df['B01001_019E']+ data_df['B01001_020E']+data_df['B01001_021E']+data_df['B01001_022E']+data_df['B01001_023E']+data_df['B01001_024E']+data_df['B01001_025E']+data_df['B01001_042E']+data_df['B01001_043E']+data_df['B01001_044E']+data_df['B01001_045E']+data_df['B01001_046E']+data_df['B01001_047E']+data_df['B01001_048E']+data_df['B01001_049E']) / data_df['B01001_001E']) * 100\n",
    "data_df['unemployed'] = (data_df['B23025_005E'] / data_df['B23025_002E']) * 100\n",
    "data_df['house75k'] = ((data_df['B19001_013E'] + data_df['B19001_014E']+ data_df['B19001_015E']+data_df['B19001_016E']+data_df['B19001_017E']) / data_df['B11001_001E']) * 100\n",
    "data_df['poverty'] = (data_df['B17010_002E'] / data_df['B01001_001E']) * 100\n",
    "data_df['rental'] = (data_df['B25003_003E'] / data_df['B25003_001E']) * 100\n",
    "data_df['mobile'] = (data_df['B25024_010E'] / data_df['B25024_001E']) * 100\n",
    "data_df['femalehous'] = (data_df['B11001_006E'] / data_df['B11001_001E']) * 100\n",
    "data_df['nodiploma'] = ((data_df['B15003_002E'] + data_df['B15003_003E']+ data_df['B15003_004E']+data_df['B15003_005E']+data_df['B15003_006E'] + data_df['B15003_007E']+ data_df['B15003_008E']+data_df['B15003_009E']+data_df['B15003_010E']+data_df['B15003_011E']+data_df['B15003_012E']+data_df['B15003_013E']+data_df['B15003_014E']+data_df['B15003_015E']+data_df['B15003_016E']) / data_df['B01001_001E']) * 100\n",
    "data_df['labor'] = (data_df['B23025_002E'] / data_df['B01001_001E']) * 100\n",
    "data_df['femalelab'] = (data_df['C24030_029E'] / data_df['C24030_001E']) * 100\n",
    "data_df['income'] = (data_df['B19301_001E'])\n",
    "data_df['medhome'] = (data_df['B25077_001E'])\n",
    "data_df['medage'] = (data_df['B01002_002E'])\n",
    "data_df['medrent'] = (data_df['B25064_001E'])\n",
    "data_df['SSrecip'] = (data_df['B19055_002E'] / data_df['B19055_001E']) * 100\n",
    "data_df['white'] = (data_df['B02001_002E'] / data_df['B01001_001E']) * 100\n",
    "data_df['black'] = (data_df['B02001_003E'] / data_df['B01001_001E']) * 100\n",
    "data_df['amerind'] = (data_df['B02001_004E'] / data_df['B01001_001E']) * 100\n",
    "data_df['asian'] = (data_df['B02001_005E'] / data_df['B01001_001E']) * 100\n",
    "data_df['hawaiian'] = (data_df['B02001_006E'] / data_df['B01001_001E']) * 100\n",
    "data_df['hispanic'] = (data_df['B03002_012E'] / data_df['B01001_001E']) * 100\n",
    "data_df['primaryemp'] = ((data_df['C24030_003E'] + data_df['C24030_028E']) / data_df['C24030_001E']) * 100\n",
    "data_df['transemp'] =  ((data_df['C24020_036E'] + data_df['C24020_072E']) / data_df['C24020_001E']) * 100\n",
    "data_df['serviceemp'] =  ((data_df['C24020_019E'] + data_df['C24020_055E']) / data_df['C24020_001E']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b661a75-5741-4d4d-ae7a-79958e1ca723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print dataframe with calculated variables\n",
    "print(data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d657fb29-ca68-485f-8341-c3437d3da838",
   "metadata": {},
   "source": [
    "# Calculating housing units per square mile\n",
    "\n",
    "The processed data is then merged dataframe into census blockgroup shapefile, which includes calculated fields such as housing units per square mile (based on field \"ALAND\"). We also replace missing values as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071abb8e-2112-4193-892c-f0e600854bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "shapefile_path = r\"path to census blockgroup shapefile\"\n",
    "output_path = r\"output path of census blockgroup shapefile with merged socioeconomic data\"\n",
    "\n",
    "# Load shapefile\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Ensure GEOID is string\n",
    "gdf['GEOID'] = gdf['GEOID'].astype(str)\n",
    "data_df['GEOID'] = data_df['GEOID'].astype(str)\n",
    "\n",
    "# Filter GEOIDs for Pinellas County\n",
    "gdf_filtered = gdf[gdf['GEOID'].str.startswith('12103')]\n",
    "data_df_filtered = data_df[data_df['GEOID'].str.startswith('12103')]\n",
    "\n",
    "# Calculate square miles from ALAND (in sq meters)\n",
    "SQM_TO_SQMILE = 3.861e-7\n",
    "gdf_filtered['SQMILES'] = gdf_filtered['ALAND'] * SQM_TO_SQMILE\n",
    "\n",
    "# Merge shapefile with ACS data\n",
    "merged_filtered_df = gdf_filtered.merge(data_df_filtered, on='GEOID', how='left')\n",
    "\n",
    "# Calculate housing units per square mile\n",
    "merged_filtered_df['housesqml'] = merged_filtered_df['B25003_001E'] / merged_filtered_df['SQMILES']\n",
    "\n",
    "# Select relevant columns for export\n",
    "columns = ['GEOID','avghouse','female','kid','elderly','unemployed','house75k','poverty','rental','mobile',\n",
    "           'femalehous','nodiploma','labor','femalelab','medage','income','medhome','medrent','SSrecip',\n",
    "           'white','black','amerind','asian','hawaiian','hispanic','primaryemp','transemp','serviceemp',\n",
    "           'housesqml']\n",
    "\n",
    "# Ensure all selected columns exist\n",
    "available_columns = [col for col in columns if col in merged_filtered_df.columns]\n",
    "\n",
    "# Add geometry for shapefile export\n",
    "final_gdf = merged_filtered_df[available_columns + ['geometry']]\n",
    "\n",
    "#replacing missing values with NaN\n",
    "final_gdf.replace(-666666666.0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c21aee-09ca-4fba-b39e-4b62d3758864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print final geodataframe\n",
    "final_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e7ef6-814b-4989-9dd2-cb7ef56ffbbc",
   "metadata": {},
   "source": [
    "# Adjusting missing values using average of neighboring polygons\n",
    "\n",
    "A function is implemented to fill in missing values in the dataset by averaging values from neighboring polygons. The GeoDataFrame with filled missing values is saved as a new shapefile. This is developed by Alec Colarusso (PhD. Student and  Web Interface & App Development Lead, University of South Florida) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e8c98-1807-4c16-9026-55b15fc321bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with missing values (represented as 0)\n",
    "missing_cols = [col for col in final_gdf.columns if (final_gdf[col] == 0).any()]\n",
    "\n",
    "def fill_missing_values(final_gdf, missing_cols, buffer_distance=0.1, max_iterations=5): #adjust buffer distance based on user requirements\n",
    "    # Convert columns to float\n",
    "    for col in missing_cols:\n",
    "        final_gdf[col] = final_gdf[col].astype(float)\n",
    "    \n",
    "    iteration = 0\n",
    "    while iteration < max_iterations:\n",
    "        final_gdf['geometry'] = final_gdf['geometry'].buffer(0)  # Clean geometry\n",
    "        \n",
    "        for idx, row in final_gdf.iterrows():\n",
    "            # Expand the current block to find neighbors\n",
    "            search_area = row.geometry.buffer(buffer_distance)\n",
    "            neighbors = final_gdf[final_gdf.geometry.intersects(search_area)].index\n",
    "\n",
    "            for col in missing_cols:\n",
    "                if final_gdf.at[idx, col] == 0:  # Treat 0 as missing value\n",
    "                    # Calculate the mean of the neighbors' values for the missing column\n",
    "                    neighbor_values = final_gdf.loc[neighbors, col]\n",
    "                    neighbor_values = neighbor_values[neighbor_values != 0]  # Exclude other missing values\n",
    "                    if not neighbor_values.empty:\n",
    "                        mean_value = neighbor_values.mean()\n",
    "                        final_gdf.at[idx, col] = mean_value\n",
    "        \n",
    "        # Check if there are still any 0 values\n",
    "        remaining_zeros = final_gdf[missing_cols].eq(0).any(axis=1)\n",
    "        if not remaining_zeros.any():\n",
    "            break\n",
    "        \n",
    "        iteration += 1\n",
    "    \n",
    "    return final_gdf\n",
    "\n",
    "# Apply the function to fill missing values\n",
    "final_gdf_filled = fill_missing_values(final_gdf, missing_cols)\n",
    "\n",
    "# Save the updated shapefile\n",
    "output_path = 'Output path of shapefile after adjusted missing values'\n",
    "final_gdf_filled.to_file(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7085c3-9bcd-4e52-9fe3-3ff1d6c3bb15",
   "metadata": {},
   "source": [
    "# SoVI Calculation\n",
    "\n",
    "We adopted the methodology by Cutter et al. (2003) to calculate the Social Vulnerability Index (SoVI) in block groups. All variables were normalized through z-score standardization. Using principal component analysis, the 28 socioeconomic variables were reduced into a few components to represent different dimensions of social vulnerability. The cardinality (+ or – sign) of the components is determined by the prior knowledge on the tendency of the phenomena to increase or decrease vulnerability. Finally, all the components with their cardinality (+,-) are placed in an additive model to generate the overall SoVI® score. Details of this method can be found in Cutter et al. (2003).\n",
    "\n",
    "Reference: Cutter, S.L.; Boruff, B.J.; Shirley, W.L. Social vulnerability to environmental hazards. Soc .Sci. Q. 2003, 84, 242–261. https://www.jstor.org/stable/42955868."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a523ca-3257-497d-b364-b81deecf5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "\n",
    "# --------------------- Configuration --------------------- #\n",
    "shapefile_path = r'output path of census blockgroup shapefile with adjusted missing values'\n",
    "loadings_output_path = r'output path of detail breakdown of factor loadings output as csv'\n",
    "variance_output_path = r'output path of detail breakdown of variance explained by each factor loadings output as csv'\n",
    "output_shapefile_path = r'output path of census blockgroup shapefile with SoVI score'\n",
    "\n",
    "# --------------------- Variables --------------------- #\n",
    "variables = [\n",
    "    'avghouse', 'female', 'kid', 'elderly', 'unemployed', 'house75k', 'poverty', 'rental',\n",
    "    'mobile', 'femalehous', 'nodiploma', 'labor', 'femalelab', 'medage', 'income',\n",
    "    'medhome', 'medrent', 'SSrecip', 'white', 'black', 'amerind', 'asian',\n",
    "    'hawaiian', 'hispanic', 'primaryemp', 'transemp', 'serviceemp', 'housesqml'\n",
    "]\n",
    "\n",
    "# --------------------- Load and Preprocess --------------------- #\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "data = gdf[variables].copy()\n",
    "data_imputed = SimpleImputer(strategy='mean').fit_transform(data)\n",
    "data_scaled = StandardScaler().fit_transform(data_imputed)\n",
    "\n",
    "# --------------------- PCA for Scree Plot --------------------- #\n",
    "pca = PCA()\n",
    "pca.fit(data_scaled)\n",
    "eigenvalues = pca.explained_variance_\n",
    "explained_var = pca.explained_variance_ratio_ * 100\n",
    "components = range(1, len(eigenvalues) + 1)\n",
    "\n",
    "# Plot Scree\n",
    "sns.set_style(\"darkgrid\")\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "ax1.plot(components, explained_var, marker='o', color='royalblue')\n",
    "ax1.set_xlabel('Principal Component', color='black')\n",
    "ax1.set_ylabel('% of variance explained', color='black')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(components, eigenvalues, marker='o', linestyle='-', color='royalblue')\n",
    "ax2.set_ylabel('Eigenvalue', color='black')\n",
    "ax2.axhline(y=1, color='red', linestyle='--')\n",
    "\n",
    "for spine in ax1.spines.values(): spine.set_color('navy')\n",
    "for spine in ax2.spines.values(): spine.set_color('navy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(scree_output_path, format='svg', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------- Factor Analysis --------------------- #\n",
    "fa_temp = FactorAnalyzer(rotation=None, method='principal')\n",
    "fa_temp.fit(data_scaled)\n",
    "eigenvals, _ = fa_temp.get_eigenvalues()\n",
    "n_factors = 9  # Based on eigenvalue > 1\n",
    "\n",
    "fa = FactorAnalyzer(n_factors=n_factors, rotation='varimax', method='principal')\n",
    "fa.fit(data_scaled)\n",
    "\n",
    "# --------------------- Factor Loadings & Variance --------------------- #\n",
    "loadings = pd.DataFrame(fa.loadings_, index=variables, columns=[f'Factor{i}' for i in range(1, n_factors + 1)])\n",
    "loadings.to_csv(loadings_output_path)\n",
    "\n",
    "variance_info = pd.DataFrame({\n",
    "    \"Factor\": [f\"Factor{i}\" for i in range(1, n_factors + 1)],\n",
    "    \"Variance Explained (%)\": fa.get_factor_variance()[1],\n",
    "    \"Cumulative Variance (%)\": fa.get_factor_variance()[2]\n",
    "})\n",
    "variance_info.to_csv(variance_output_path, index=False)\n",
    "\n",
    "# --------------------- Compute Factor Scores --------------------- #\n",
    "factor_scores = fa.transform(data_scaled)\n",
    "factor_scores_df = pd.DataFrame(factor_scores, columns=[f'Factor{i}' for i in range(1, n_factors + 1)])\n",
    "factor_scores_df = (factor_scores_df - factor_scores_df.mean()) / factor_scores_df.std()\n",
    "\n",
    "# --------------------- Apply Directional Signs --------------------- #\n",
    "# Signs derived from interpretation\n",
    "signs = {\n",
    "    'Factor1': '+',  # Elderly Age Dependency\n",
    "    'Factor2': '-',  # Affluence and Housing Investment\n",
    "    'Factor3': '+',  # Racial and Economic Marginalization\n",
    "    'Factor4': '+',  # Gender-Based Structural Inequality\n",
    "    'Factor5': '+',  # Urban Density and Rental Exposure\n",
    "    'Factor6': '+',  # Educational Deficits and Minority Status\n",
    "    'Factor7': '+',  # Native American and Hispanic Presence\n",
    "    'Factor8': '+',  # Pacific Islander Concentration\n",
    "    'Factor9': '+'   # Child Dependents and Precarious Labor\n",
    "}\n",
    "\n",
    "for factor, sign in signs.items():\n",
    "    if sign == '-':\n",
    "        factor_scores_df[factor] *= -1\n",
    "\n",
    "# --------------------- Compute Final SoVI Score --------------------- #\n",
    "gdf['SoVI'] = factor_scores_df.sum(axis=1)\n",
    "\n",
    "# --------------------- Classification --------------------- #\n",
    "# Quantile-based\n",
    "gdf['SoVI_Q'] = pd.qcut(gdf['SoVI'], q=5, labels=False)\n",
    "gdf['SoVI_QS'] = pd.qcut(gdf['SoVI'], q=5, labels=['Very Low', 'Low', 'Moderate', 'High', 'Very High'])\n",
    "\n",
    "# SD-based\n",
    "mean_sovi = gdf['SoVI'].mean()\n",
    "std_sovi = gdf['SoVI'].std()\n",
    "bins_sd = [\n",
    "    gdf['SoVI'].min(),\n",
    "    mean_sovi - 1.5 * std_sovi,\n",
    "    mean_sovi - 0.5 * std_sovi,\n",
    "    mean_sovi + 0.5 * std_sovi,\n",
    "    mean_sovi + 1.5 * std_sovi,\n",
    "    gdf['SoVI'].max()\n",
    "]\n",
    "gdf['SoVI_SD'] = gdf['SoVI']\n",
    "gdf['SoVI_SDS'] = pd.cut(gdf['SoVI'], bins=bins_sd, labels=['Very Low', 'Low', 'Moderate', 'High', 'Very High'], include_lowest=True)\n",
    "\n",
    "# Convert labels to string to ensure compatibility with shapefile\n",
    "gdf['SoVI_QS'] = gdf['SoVI_QS'].astype(str)\n",
    "gdf['SoVI_SDS'] = gdf['SoVI_SDS'].astype(str)\n",
    "\n",
    "# --------------------- Export Final Shapefile --------------------- #\n",
    "gdf.to_file(output_shapefile_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
